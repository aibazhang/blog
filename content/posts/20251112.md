---
title: "GKEでrollout restartする際に起きる503エラーを解消した"
date: 2025-11-12T08:33:24+00:00
tags: ["日本語"]
categories: ["tech"]
author: "Me"
editPost:
    URL: "https://github.com/aibazhang/blog/tree/master/content"
    Text: "Suggest Changes"
    appendFilePath: true
---

前日、GKEでdeploymentに対してrollout restartを実行すると、10s前後の503エラーが起きる事象を気づいた。呼び出す側はエラー処理しているはずなので、大きな問題ではなかったが、オンプレのクラスタでは同じ事象が起きないので、原因を調査した。

環境
- GKE: 1.34.1-gke.1829001
- Networking: Gateway API

## 再現

以下のスクリプトを実行させながら、`kubectl rollout restart deployment/your-app`を実行する。

```bash
START=$(date +%s.%N)
while true; do
  curl -s https://your-app.example.com/
  END=$(date +%s.%N)
  ELAPSED=$(echo "$END - $START" | bc)
  echo -e "\nElapsed time: ${ELAPSED}s"
  sleep 0.1
done
```

10s前後、503系のエラーが起きると確認できる。
```
Elapsed time: 21.341049000s
OK
Elapsed time: 21.530590000s
OK
Elapsed time: 21.730951000s
upstream connect error or disconnect/reset before headers. reset reason: remote connection failure, transport failure reason: delayed connect error: Connection refused
Elapsed time: 21.938160000s
upstream connect error or disconnect/reset before headers. reset reason: remote connection failure, transport failure reason: delayed connect error: Connection refused
Elapsed time: 22.135801000s
upstream connect error or disconnect/reset before headers. reset reason: connection timeout
Elapsed time: 26.838277000s
upstream connect error or disconnect/reset before headers. reset reason: connection timeout
Elapsed time: 31.532648000s
OK
Elapsed time: 31.740313000s
```

## `503 upstream connect error or disconnect/reset before headers`

処理中がリクエストがあるのに、backend serviceが消えたら、このエラーになる。
preStop hookを追加すると解消できた。

```yaml
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "sleep 20"]
```

ただし、イメージに`/bin/sh`と`sleep`の存在が前提なので、[distroless](https://github.com/GoogleContainerTools/distroless)を使っている場合、
baseとなるイメージを変えるか、sleep用のendpointを実装するかなどの工夫が必要そう。

> I1113 11:00:23.813868   53249 warnings.go:110] "Warning: autopilot-workload-defaulter:Autopilot added tolerations matching: cloud.google.com/gke-spot"
I1113 11:00:23.813935   53249 warnings.go:110] "Warning: autopilot-default-resources-mutator:The max supported TerminationGracePeriodSeconds is 25 seconds when using toleration of cloud.google.com/gke-spot=true:NoSchedule. Defaulting down from configured 30 seconds to 25 seconds."

spot nodeの場合、TerminationGracePeriodSecondsはそもそも25sしかないので、大きな値にしても効果がない。

余談だが、preStop Hookのeventはpodから直接確認できないので、`kubectl event`を使う必要がある。これを知らずにdebugするのに時間がかかった。

## `503 no healthy upstream`

上記エラーを解決したが、次に`no healthy upstream`というエラーが出た。

[ドキュメント](https://docs.cloud.google.com/kubernetes-engine/docs/how-to/deploying-gateways#no-healthy-upstream)を調べたら、以下の記述があった。

> This error message indicates that the health check prober cannot find healthy backend services

podは立ち上がったけど、LBがbackend serviceへのhealth checkが失敗している。（k8sのreadiness probeなどとは別の話）

podのeventを見たら、NEGがattachされていないぞというwarningがあった。

```
NEG is not attached to any BackendService with health checking. Marking condition "cloud.google.com/load-balancer-neg-ready" to True.
```

Ingress APIにドキュメントの[know issues](https://docs.cloud.google.com/kubernetes-engine/docs/how-to/container-native-load-balancing#neg_readiness_gate_race_condition)にも書いてあるように、NEG controllerはhealth checkがそもそもないか、health checkが成功したかを判別できないので、podをreadyにマークしてしまう。Ingress APIのドキュメントではあるが、Gateway APIも同じissueが存在している。

このドキュメントに書いてある「複数のbackendsを用意しよう」「rolling update strategyを変えよう」といったResolutionsが全然回答になっていない。NEGに複数のbackendsを用意しても、結局NEGのattachが完了するまでに待たないといけない。


podのreadyを、NEGがattachされるまでに遅延させる方法があるかを調べたら、[minReadySeconds](https://kubernetes.io/blog/2021/08/27/minreadyseconds-statefulsets)という機能を見つけた。`minReadySeconds: 60`を設定したら、`503 no healthy upstream`が見事に解消された。


> Another case, where minReadySeconds helps is when using LoadBalancer Services with cloud providers. Since minReadySeconds adds latency after a Pod is Ready, it provides buffer time to prevent killing pods in rotation before new pods show up

今まであまり使う機会がなかったが、まさにこのユースケースのために開発された機能。
