---
title: "AirflowからDataformにdata_interval_endなどのcontext変数を渡す方法"
date: 2023-05-24T00:00:00+00:00
tags: ["tech", "日本語", "airflow", "gcp", "python"]
author: "Me"
showToc: true
TocOpen: false
draft: false
hidemeta: false
comments: true
description: ""

disableHLJS: true # to disable highlightjs
disableShare: true
disableHLJS: false
hideSummary: false
searchHidden: true
# ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: false
# ShowWordCount: true
ShowRssButtonInSectionTermList: true
UseHugoToc: true
cover:
    image: "<image path/url>" # image path/url
    alt: "<alt text>" # alt text
    caption: "<text>" # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: true # only hide on current single page
editPost:
    URL: "https://github.com/aibazhang/blog/tree/master/content"
    Text: "Suggest Changes" # edit text
    appendFilePath: true # to append file path to Edit link
---


先日GCPのDataformがGAリリースされました。
せっかくなので、まずAirflowにある既存ワークフローの一部をDataformで書き換えようと思いました。

## AirflowからDataformをトリッガーする

ドキュメントを調べると、AirflowからDataformをトリッガーするoperatorはすでに存在しています。
https://cloud.google.com/dataform/docs/schedule-executions-composer#create_an_airflow_dag_that_schedules_workflow_invocations

簡単にまとめると
1. `DataformCreateCompilationResultOperator`: sqlxをsqlにコンパイルする
2. `DataformCreateWorkflowInvocationOperator`: sqlを実行する

しかし、どのようにAirflowからDataformへ変数を渡すかについてはドキュメントに記載されていません。

## Dataformに変数を渡す

まず、Dataformの設定ファイルdataform.jsonに変数varsを追加しておきましょう。
```json
{
  "defaultSchema": "dataform",
  "assertionSchema": "dataform_assertions",
  "warehouse": "bigquery",
  "defaultDatabase": "project-stg",
  "defaultLocation": "asia-northeast1",
  "vars": {
    "bq_suffix": "_stg",
    "execution_date": "2023-05-24"
  }
}
```

DataformCreateCompilationResultOperatorのソースを調べてみたところ、compilation_resultという引数があることを発見しました。

https://github.com/apache/airflow/blob/739e6b5d775412f987a3ff5fb71c51fbb7051a89/airflow/providers/google/cloud/operators/dataform.py#LL73C29-L73C46

compilation_resultの中身を確認するため、APIの詳細を調べました。
https://cloud.google.com/dataform/reference/rest/v1beta1/CodeCompilationConfig

`CodeCompilationConfig`内に`vars`という変数を指定できるようです。

```json
{
  "defaultDatabase": string,
  "defaultSchema": string,
  "defaultLocation": string,
  "assertionSchema": string,
  "vars": {
    string: string,
    ...
  },
  "databaseSuffix": string,
  "schemaSuffix": string,
  "tablePrefix": string
}
```

BigQueryのsuffixを`code_compilation_config`の`vars`へ渡してみたら問題なく実行できました。ちなみに、Dataform側からは`dataform.projectConfig.vars.bq_suffix`で変数を呼び出せます。

```python
DataformCreateCompilationResultOperator(
    task_id="create_compilation_result",
    project_id=PROJECT_ID,
    region=REGION,
    repository_id=REPOSITORY_ID,
    compilation_result={
        "git_commitish": GIT_COMMITISH,
        "code_compilation_config": {
            "vars": {
                "bq_suffix": "_stg",
            }
        },
    },
)
```

## Dataformにcontext変数を渡す

増分処理する際によく`data_interval_end`などの[context変数](https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html#variables)を利用して当日の差分だけ取り入れます。
しかし、`DataformCreateCompilationResultOperator`では`template_fields`が実装されていないため、直接`{{ data_interval_end }}`のようなjinjaテンプレートを渡すことはできません。


**[TaskFlow](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html)で`DataformCreateCompilationResultOperator`をラッピングすれば前述の問題を解決できます**。`data_interval_end`は`context`から取得します。ポイントとしては`DataformCreateCompilationResultOperator`を返す際に`execute()`を呼び出す必要があります。

```python
from airflow.decorators import task

@task()
def create_compilation_result(**context):
    execute_date = (
        context["data_interval_end"].in_timezone("Asia/Tokyo").strftime("%Y-%m-%d")
    )
    return DataformCreateCompilationResultOperator(
        task_id="create_compilation_result",
        project_id=PROJECT_ID,
        region=REGION,
        repository_id=REPOSITORY_ID,
        compilation_result={
            "git_commitish": GIT_COMMITISH,
            "code_compilation_config": {
                "vars": {
                    "execute_date": execute_date,
                    "bq_suffix": Variable.get("bq_suffix"),
                }
            },
        },
    ).execute(context=context)
```

最終的なDAGは以下のようになります。

```python 
from datetime import datetime

from airflow import models
from airflow.decorators import task
from airflow.models import Variable
from airflow.providers.google.cloud.operators.dataform import (
    DataformCreateCompilationResultOperator,
    DataformCreateWorkflowInvocationOperator,
)

DAG_ID = "dataform_demo"
PROJECT_ID = "project-stg"
REPOSITORY_ID = "dataform-demo"
REGION = "asia-northeast1"
GIT_COMMITISH = "main"

@task()
def create_compilation_result(**context):
    execute_date = (
        context["data_interval_end"].in_timezone("Asia/Tokyo").strftime("%Y-%m-%d")
    )
    return DataformCreateCompilationResultOperator(
        task_id="create_compilation_result",
        project_id=PROJECT_ID,
        region=REGION,
        repository_id=REPOSITORY_ID,
        compilation_result={
            "git_commitish": GIT_COMMITISH,
            "code_compilation_config": {
                "vars": {
                    "execute_date": execute_date,
                    "bq_suffix": Variable.get("bq_suffix"),
                }
            },
        },
    ).execute(context=context)

with models.DAG(
    DAG_ID,
    schedule_interval="@once",
    start_date=datetime(2022, 1, 1),
    catchup=False,
    tags=["dataform"],
) as dag:

    create_workflow_invocation = DataformCreateWorkflowInvocationOperator(
        task_id="create_workflow_invocation",
        project_id=PROJECT_ID,
        region=REGION,
        repository_id=REPOSITORY_ID,
        workflow_invocation={
            "compilation_result": "{{ task_instance.xcom_pull('create_compilation_result')['name'] }}"
        },
    )

    create_compilation_result() >> create_workflow_invocation
```

以上、AirflowからDataformにdata_interval_endなどのcontext変数を渡す方法でした。